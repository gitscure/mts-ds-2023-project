{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание ШАД МТС по теме Линейная регрессия 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "По определению мы знаем, что $\\displaystyle R^2 = \\frac{ESS}{TSS}$,\n",
    "\n",
    "а также, что $\\displaystyle Corr(y_i, \\hat{y_i}) = \\frac{Cov(y_i, \\hat{y_i})}{\\sigma_y \\sigma_{\\hat{y}}}$.\n",
    "\n",
    "Докажите следующее свойство: $\\displaystyle R^2 = Corr^2(y_i, \\hat{y_i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доказательство\n",
    "\n",
    "$\\displaystyle Corr(y_i, \\hat{y_i}) = \\frac{Cov(y_i, \\hat{y_i})}{\\sigma_y \\sigma_{\\hat{y}}}$\n",
    "\n",
    "$\\displaystyle Corr^2(y_i, \\hat{y_i}) = \\frac{Cov(y_i, \\hat{y_i})Cov(y_i, \\hat{y_i})}{\\sigma_y^2 \\sigma_{\\hat{y}}^2}$\n",
    "\n",
    "Т.к. $\\displaystyle y_i = \\hat{y_i} + \\hat{\\epsilon_i}$, получаем, что\n",
    "\n",
    "$\\displaystyle Corr^2(y_i, \\hat{y_i}) = \\frac{Cov(\\hat{y_i} + \\hat{\\epsilon_i}, \\hat{y_i})Cov(\\hat{y_i} + \\hat{\\epsilon_i}, \\hat{y_i})}{\\sigma_y^2 \\sigma_{\\hat{y}}^2}$\n",
    "\n",
    "Воспользуемся свойством, что $\\displaystyle Cov(x, (y + \\alpha)) = Cov(x, y) + Cov(x, \\alpha)$\n",
    "\n",
    "$\\displaystyle Corr^2(y_i, \\hat{y_i}) = \\frac{(Cov(\\hat{y_i}, \\hat{y_i}) + Cov(\\hat{y_i}, \\hat{\\epsilon_i}))(Cov(\\hat{y_i}, \\hat{y_i}) + Cov(\\hat{y_i}, \\hat{\\epsilon_i}))}{\\sigma_y^2 \\sigma_{\\hat{y}}^2}$\n",
    "\n",
    "Т.к. нет зависимости $\\displaystyle \\hat{y_i}$ от $\\displaystyle \\hat{\\epsilon_i}$, то $\\displaystyle Cov(\\hat{y_i}, \\hat{\\epsilon_i}) = 0$\n",
    "\n",
    "$\\displaystyle Corr^2(y_i, \\hat{y_i}) = \\frac{Cov(\\hat{y_i}, \\hat{y_i})Cov(\\hat{y_i}, \\hat{y_i})}{\\sigma_y^2 \\sigma_{\\hat{y}}^2}$\n",
    "\n",
    "$\\displaystyle Cov(\\hat{y_i}, \\hat{y_i}) = \\sigma_{\\hat{y}}^2$, следовательно\n",
    "\n",
    "$\\displaystyle Corr^2(y_i, \\hat{y_i}) = \\frac{\\sigma_{\\hat{y}}^2\\sigma_{\\hat{y}}^2}{\\sigma_y^2 \\sigma_{\\hat{y}}^2} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$\n",
    "\n",
    "Исходя из определения дисперсии, получаем, что:\n",
    "\n",
    "$\\displaystyle Corr^2(y_i, \\hat{y_i}) = \\frac{\\displaystyle \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i} - \\bar{y})^2}{\\displaystyle \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2} = \\frac{ESS}{TSS} = R^2$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "На лекции мы показали, что оценки коэффициентов в матричной форме можно вычислить по следующей формуле: $\\displaystyle \\hat{\\beta} = (X^T X)^{-1} X^T y$.\n",
    "\n",
    "Также мы знаем, что оценки МНК несмещенные: $\\displaystyle E[\\hat{\\beta}_j | x] = \\beta_j$.\n",
    "\n",
    "Необходимо:\n",
    "1. Выразить оценки коэффициентов $\\displaystyle \\hat{\\beta}$ через $\\displaystyle \\beta$ в матричной форме (по аналогии с выводом Теоремы 1 из лекции).\n",
    "2. Доказать свойство несмещенности коэффициентов $\\displaystyle \\hat{\\beta}$ в матричной форме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение\n",
    "\n",
    "1. Выразим оценки коэффициентов $\\displaystyle \\hat{\\beta}$ через $\\displaystyle \\beta$ в матричной форме.\n",
    "\n",
    "В модели линейной регрессии $\\displaystyle y = X\\beta + \\epsilon$, где $y$ - вектор ответов, $X$ - матрица признаков, $\\beta$ - вектор истинных коэффициентов, а $\\epsilon$ - вектор ошибок.\n",
    "\n",
    "Тогда, подставив $y$ из модели в формулу для $\\hat{\\beta}$, получим:\n",
    "\n",
    "$\\displaystyle \\hat{\\beta} = (X^T X)^{-1} X^T (X\\beta + \\epsilon) = \\hat{\\beta} = (X^T X)^{-1} X^T X\\beta + (X^T X)^{-1} X^T \\epsilon$\n",
    "\n",
    "Произведение $\\displaystyle (X^T X)^{-1} X^T X$ дает единичную матрицу. Тогда получаем:\n",
    "\n",
    "$\\displaystyle \\hat{\\beta} = \\beta + (X^T X)^{-1} X^T \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Докажем свойство несмещенности коэффициентов $\\displaystyle \\hat{\\beta}$ в матричной форме.\n",
    "\n",
    "Несмещенность означает, что математическое ожидание оценки коэффициента равно его истинному значению. То есть, нужно показать, что $E[\\hat{\\beta}|X] = \\beta$.\n",
    "\n",
    "$\\displaystyle E[\\hat{\\beta}|X] = E[\\beta + (X^T X)^{-1} X^T \\epsilon|X]$\n",
    "\n",
    "Так как $\\beta$ - это константа, то $E[\\beta|X] = \\beta$. Кроме того, предполагаем, что ошибки $\\epsilon$ имеют нулевое среднее, соответственно $\\displaystyle E[\\epsilon|X] = 0$. Тогда получаем:\n",
    "\n",
    "$\\displaystyle E[\\hat{\\beta}|X] = \\beta + (X^T X)^{-1} X^T E[\\epsilon|X] = \\beta + 0 = \\beta$\n",
    "\n",
    "Таким образом, оценки коэффициентов $\\hat{\\beta}$ в матричной форме являются несмещенными.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "Докажите, что $\\displaystyle \\sum_{i=1}^n (y_i - \\bar{y})^2 = RSS + ESS + 2 \\sum_{i=1}^n \\hat{\\epsilon_i} (\\hat{y_i} - \\bar{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доказательство\n",
    "\n",
    "$\\displaystyle \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n [(y_i - \\hat{y_i}) + (\\hat{y_i} - \\bar{y})]^2 = \\sum_{i=1}^n [(y_i - \\hat{y_i})^2 + 2(y_i - \\hat{y_i})(\\hat{y_i} - \\bar{y}) + (\\hat{y_i} - \\bar{y})^2] = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 + 2\\sum_{i=1}^n (y_i - \\hat{y_i})(\\hat{y_i} - \\bar{y}) + \\sum_{i=1}^n (\\hat{y_i} - \\bar{y})^2$\n",
    "\n",
    "Т.к. $\\displaystyle \\; RSS = \\sum_{i=1}^n (y_i - \\hat{y_i})^2$,\n",
    "$\\displaystyle \\; ESS = \\sum_{i=1}^n (\\hat{y_i} - \\bar{y})^2$,\n",
    "$\\displaystyle \\; \\hat{\\epsilon_i} = y_i - \\hat{y_i}$, получаем:\n",
    "\n",
    "$\\displaystyle \\sum_{i=1}^n (y_i - \\bar{y})^2 = RSS + 2\\sum_{i=1}^n \\hat{\\epsilon_i} (\\hat{y_i} - \\bar{y}) + ESS$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "\n",
    "Используя предпосылку Гаусса-Маркова №4 (Гомоскедантичность стандартного отклонения ошибки) в матричной форме: $\\displaystyle Var(\\epsilon | X) = \\sigma^2 I_n$, где $\\displaystyle I_n$ - единичная матрица,\n",
    "\n",
    "докажите, что $\\displaystyle Var(\\hat{\\beta} | X) = \\sigma^2 (X^T X)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доказательство\n",
    "\n",
    "В задании №2 мы доказали, что $\\displaystyle \\hat{\\beta} = \\beta + (X^T X)^{-1} X^T \\epsilon$\n",
    "\n",
    "Найдем дисперсию:\n",
    "\n",
    "$\\displaystyle Var(\\hat{\\beta} | X) = Var(\\beta + (X^T X)^{-1} X^T \\epsilon | X)$\n",
    "\n",
    "Т.к. $\\displaystyle Var(С) = 0$, то\n",
    "\n",
    "$\\displaystyle Var(\\hat{\\beta} | X) = Var((X^T X)^{-1} X^T \\epsilon | X) = (X^T X)^{-1} X^T Var(\\epsilon | X) (X^T X)^{-1} X$\n",
    "\n",
    "Воспользовавшись предпосылкой Гаусса-Маркова $\\displaystyle Var(\\epsilon | X) = \\sigma^2 I_n$, где $\\displaystyle I_n$ получаем\n",
    "\n",
    "$\\displaystyle Var(\\hat{\\beta} | X) = (X^T X)^{-1} X^T \\sigma^2 I_n (X^T X)^{-1} X = \\sigma^2 (X^T X)^{-1}$, т.к.\n",
    "\n",
    "$\\displaystyle (X^T X)^{-1} X^T X$ - единичная матрица"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
